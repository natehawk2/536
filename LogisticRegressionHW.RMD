---
title: "Targeted Marketing"
author: "Nathan Hawkins"
date: "11/5/2021"
output:
  pdf_document: default
  html_document: default
---

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(crosstable)

mark <- read.csv("TargetedMarketing.csv", header = TRUE, sep = ";")

mark$job = as.factor(mark$job)
mark$marital = as.factor(mark$marital)
mark$education = as.factor(mark$education)
mark$default = as.factor(mark$default)
mark$housing = as.factor(mark$housing)
mark$loan = as.factor(mark$loan)
mark$contact = as.factor(mark$contact)
mark$month = as.factor(mark$month)
mark$day_of_week = as.factor(mark$day_of_week)
mark$poutcome = as.factor(mark$poutcome)
mark$y = ifelse(mark$y == "no", 0, 1)
mark$y = as.factor(mark$y)
mark$pdays <- ifelse(mark$pdays == "999", 0,
                     ifelse(mark$pdays < 5, 1, 
                          ifelse(mark$pdays > 4 & mark$pdays < 10, 2, 
                                 ifelse(mark$pdays > 9 & mark$pdays < 15, 3, 
                                        ifelse(mark$pdays > 14 & mark$pdays < 20, 4, 
                                               ifelse(mark$pdays > 19, 5, 6))))))

```

# Introduction


Targeted marketing campaigns are essential for companies that are trying to optimize their outreach efforts. To reach a target audience, marketing directors should have a good understanding of which groups could be profitable. In this analysis, we will attempt to understand which characteristics make members of a bank more likely to open an account with a credit card. At our disposal we have a data set with demographic information for 41,000 bank members and whether or not they opened a new account. In addition to understanding which variables are most conducive to opening a new account, we are also interested in understanding whether social media or personal contact are more effective in marketing, and if repeated contacting increases the likelihood of a person opening an account. First we will explore the data. Below we show cross tables of a view variables of interest:


```{r echo = FALSE}
par(mfrow= c(2,1))

mark1 <- mark
mark1$previous <- as.factor(mark$previous)

crosstable(mark, c(contact), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark1, c(previous), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)
```

Initially it appears that members that are contacted over the phone rather than over social media are less likely to open a new account. Also, members that have been contacted more appear more likely to open an account. Because we are dealing with a binary response variable (the member either opens an account or doesn't) we will need to use classification techniques instead of regression. To do this, we will use logistic regression. A regular linear model would not be able to classify this response variable. Machine learning models will do a poor job at understanding the relationships between variables and response. Because our primary interest in this analysis is inference instead of prediction, we will use classic logistic regression.

# Models

We will consider both a logit and probit logistic regression model in this analysis. Both models will be appropriate for the binary response variable in this analysis. These models will likely have similar performance but different interpretations. We will evaluate the performance of both of these models and decide which one will be better. These models will both give us estimates for the effects of the variables on responses and so we will be able to answer our questions of interest.

Both of these models have inherent assumptions that must be met for the analysis to be valid. First we must assume that our data are independent. This has to do with the method of obtaining the data. We also assume monotonicity in the independent variables. That is to say that there will either be an increase or decrease in the response variable as you increase the independent variable. If a variable increases and then decreases, this violates the monotonicity assumption. We will check these assumptions after selecting the model.

## Model Summary

We run a 10-fold cross validation to compare model performance and show the results below.
```{r echo = FALSE}
table <- data.frame("Model" = c("Logit", "Probit"), "Classification" = c(1-0.31,1-0.30), "AUC" = c(0.76, 0.76), "Sensitivity" = c(0.67, 0.66), "Specificity" = c(0.69,0.70))
knitr::kable(table, caption = "Out-of-sample model fits"
             #col.names = c("Model", "RMSE", "R Squared")
             )
```

The cross validation shows that these models are very similar in predictive performance. The sensitivity or true positive rate of the logit model is a little bit better. This means that it is a little bit better at correctly predicting a member sign up. The specificity or true negative rate of the probit model is slightly better. This means it outperforms the logit model at correctly predicting a member not signing up. 

We will now look at the in-sample fit:
```{r}
table <- data.frame("Model" = c("Logit", "Probit"), "Classification" = c(1-0.31,1-0.31), "AUC" = c(0.76, 0.76), "Sensitivity" = c(0.67, 0.68), "Specificity" = c(0.69,0.69))
knitr::kable(table, caption = "In-sample model fits")
```
The in-sample fits are nearly identical between the two models. So, from both a out-of-sample and in-sample fit view, these two models are almost identical. So we will continue with a logit model because it is more interpretable and we will better understand the independent variables than with the probit model.




```{r include = FALSE}
library(ggplot2)
library(dplyr)
scatter.smooth(jitter(mark$previous,amount=.5),jitter(as.numeric(mark$y)-1,amount=.1),pch=19,cex=.5,xlab="Previous",ylab="Diabetes", main = "Diabetes vs. Age Density Graph")


library(forcats)
mark <- mark %>%
  mutate(day_of_week = fct_relevel(day_of_week, 
            "mon", "tue", "wed", 
            "thu", "fri"))

mark$month
mark <- mark %>%
  mutate(month = fct_relevel(month, 
            "mar", 
            "apr", "may", "jun",
            "jul", "aug", "sep", 
            "oct", "nov", "dec"))

library(crosstable)
library(dplyr)
crosstable(mark, c(y), by=day_of_week, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(day_of_week, marital), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(job), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(marital), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(education), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(month), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(default), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(contact), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

crosstable(mark, c(loan), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)

mark1 <- mark
mark1$previous <- as.factor(mark$previous)
crosstable(mark1, c(previous), by=y, percent_digits=0) %>% 
  as_flextable(keep_id=FALSE)


```



```{r include = FALSE}
mylogit <- glm(y ~ age + default + contact + month + campaign + as.factor(pdays), data = mark, family = binomial(link = "logit"))
BIC(mylogit)
```



```{r include=FALSE}
mylogit <- glm(y ~ job + default + contact + month + campaign + as.factor(pdays) , data = mark, family = binomial(link = "logit"))
#summary(mylogit)
summary(mylogit)
AIC(mylogit)
BIC(mylogit)
# Check in sample fit
y.hat = ifelse(mylogit$fitted.values < 0.097,0,1)
table(y.hat)
sensitivity.logit =  mean(y.hat[mark$y == 1] == 1)
specificity.logit = mean(y.hat[mark$y == 0] == 0)
predictions <- predict.glm(mylogit, type = "response")

library(pROC)
auc(mark$y, predictions)
mean(y.hat == mark$y)


mylogit <- glm(y ~ job + default + contact + month + campaign + as.factor(pdays) , data = mark, family = binomial(link = "probit"))
#summary(mylogit)
summary(mylogit)
AIC(mylogit)
BIC(mylogit)
# Check in sample fit
y.hat = ifelse(mylogit$fitted.values < 0.097,0,1)
table(y.hat)
sensitivity.probit =  mean(y.hat[mark$y == 1] == 1)
specificity.probit = mean(y.hat[mark$y == 0] == 0)
auc(mark$y, predictions)
mean(y.hat == mark$y)

```



```{r include = FALSE}
Threshold = seq(0,1,by=0.001)
false.negative.rate <- false.positive.rate <- true.positive.rate <- error.rate <- (0*Threshold-99)
model = mylogit
y = mark$y
hist(model$fitted.values)
for(i in 1:length(Threshold)){
  y.hat = ifelse(model$fitted.values < Threshold[i],0,1)
  error.rate[i] = mean(y!=y.hat)
  TP = sum(y==1 & y.hat==1)
  TN = sum(y==0 & y.hat==0)
  FP = sum(y==0 & y.hat==1)
  FN = sum(y==1 & y.hat==0)
  false.negative.rate[i] = FN/(FN+TP) # 1-sensitivity
  false.positive.rate[i] = FP/(FP+TN) # 1-specificity
}

## Errors plot from slide
plot(Threshold,error.rate,ylim=c(0,1),ylab='Error Rate',type='l',lwd=2)
lines(Threshold,false.negative.rate,col=4,lty=2,lwd=2)
lines(Threshold,false.positive.rate,col=2,lty=3,lwd=2)
abline(v = 0.097)

## ROC
plot(false.positive.rate,1 - false.negative.rate
     ,xlab = "False Positive Rate (1-Specificity)",ylab = "True Positive Rate (Sensitivity)"
     ,col=4,type='l',lwd=2
)
abline(0,1,lty=3)

best.threshold = which.min(abs(false.negative.rate - false.positive.rate))

1-error.rate[best.threshold]
false.negative.rate[best.threshold]
false.positive.rate[best.threshold]

```

```{r include = FALSE}
predictions <- predict.glm(mylogit, type = "response")

library(pROC)
auc(mark$y, predictions)
```



# Cross validate
```{r}
# best.threshold <- 0.097
# K = 10
# possibilities = 1:nrow(mark)
# this.many = round(nrow(mark)/K)
# 
# splits <- list()
# already.used <- NA
# for(i in 2:K){
#   samp <- sample(possibilities, this.many, replace = FALSE)
#   splits[[i]] <- samp
#   possibilities = possibilities[!(possibilities %in% splits[[i]]) ]
# }
# splits[[1]] = possibilities
# 
# error.rate = NA
# auc.logit <- NA
# sens.logit <- NA
# spec.logit <- NA
# 
# for(i in 1:K){
#   train.data <- mark[-splits[[i]],]
#   test.data <- mark[splits[[i]],]
#   mylogit <- glm(y ~ job + default + contact + month + campaign + as.factor(pdays), data = train.data, family = binomial(link = "logit"))
#   predictions <- predict(mylogit, type = "response", newdata = test.data)
#   preds = ifelse(predictions > best.threshold, 1, 0)
# 
#   error.rate[i] <- sum(preds != test.data$y)/nrow(test.data)
#   auc.logit[i] <- auc(test.data$y, predictions)
#   sens.logit[i] <- mean(preds[test.data$y == 1] == 1)
#   spec.logit[i] <- mean(preds[test.data$y == 0] == 0)
# }
# 
# mean(error.rate)
# mean(auc.logit)
# mean(sens.logit)
# mean(spec.logit)
# 
# 
# error.rate.prob = NA
# auc.probit <- NA
# sens.probit <- NA
# spec.probit <- NA
# 
# 
# for(i in 1:K){
#   train.data <- mark[-splits[[i]],]
#   test.data <- mark[splits[[i]],]
#   mylogit <- glm(y ~ job + default + contact + month + campaign + as.factor(pdays), data = train.data, family = binomial(link = "probit"))
#   predictions <- predict(mylogit, type = "response", newdata = test.data)
#   preds = ifelse(predictions > best.threshold, 1, 0)
# 
#   error.rate.prob[i] <- sum(preds != test.data$y)/nrow(test.data)
#   auc.probit[i] <- auc(test.data$y, predictions)
#   sens.probit[i] <- mean(preds[test.data$y == 1] == 1)
#   spec.probit[i] <- mean(preds[test.data$y == 0] == 0)
# }
# 
# mean(error.rate.prob)
# mean(auc.probit)
# mean(sens.probit)
# mean(spec.probit)

```


