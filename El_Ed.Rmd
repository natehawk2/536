---
title: "HW 3 Neural Nets"
author: "Nathan Hawkins"
date: "10/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r include = FALSE}
library(car)
school = read.csv("school.txt", sep = " ")
head(school)

```

```{r echo = FALSE}
library(corrplot)
corrplot(cor(school), method = "number")

library(ggplot2)
ggplot(data = school, mapping = aes(x = Score, y = Income)) + 
  geom_point()

ggplot(data = school, mapping = aes(x = Score, y = Lunch)) + 
  geom_point()

```


```{r include = FALSE}

library(bestglm)
bestglm(school[,c(2:7, 1)], IC = "AIC", method = "exhaustive")


model <- Score~.
fit <- lm(model, school)
test <- olsrr::ols_step_all_possible(fit)
#View(test)



#This is the best model
school.lm <- lm(Score~ Lunch + Computer + English + Income, data = school)
summary(school.lm)
```


# Modeling and Methods

In order to  answer our questions of interest, we attemt multiple different modeling techniques. We attempt a linear model, bagging, boosting, random forest, and neural network. For the bagging boosting and random forest models we found the optimal number of trees to get the lowest out-of-sample root mean squared error. To find the best neural network we optimized the number of nodes and layers on root mean squared error. The best linear model was selected based on AIC. The following table compares out-of-sample rmse and r squared for the five models.

```{r}
# RMSE/R^2 Comparison
table <- data.frame("Method" = c("Linear Model", "Bagging (160)", "Random Forest (50)", "Boosting (100)"), "R Squared" = c(0.765, 0.766, 0.751, 0.733), "RMSE" = c(19.6, 19.2, 20.2, 20.9))
knitr::kable(table)
```

We can see that the bagging model has both the best R.squread and the best RMSE. However all of the models are very similar given the scale of the data. The RMSE of 19.6 for the linear model is roughly the same as the RMSE of the bagging model. Because they are very similar we chose to continue with a linear model because this gives us the greatest interpretability of the variables. A disadvantage of the bagging, random forest, boosting, and neural networks is that it is difficult to understand what effect the variables have on the response variable. Since performance is almost the same across models, we selct the one that is most interpretable: linear model.

Model: $y_i$ = $\beta_0$ + $\beta_1$$x_{i1}$ +  $\beta_2$$x_{i2}$ + $\beta_3$$x_{i3}$ + $\beta_4$$x_{i4}$ + $\epsilon_i$ or

$y_i$ = $\beta_0$ + $\beta_1$Lunch +  $\beta_2$Computer +  $\beta_3$English + $\beta_4$Income + $\epsilon_i$

$\epsilon \sim \mathcal{N}(\mu,\,\sigma^{2})$

$\beta_0$: The test score of a school with no computers, no students qualifying for reduced-price lunch, no income, and no students learning English.

$\beta_1$: This is the effect that increase by one percent in reduced-lunch students has on the price of test scores holding all else constant

$\beta_2$: This is the effect that an increase of 1 computer has on the test scores holding all else constant

$\beta_3$: The effect of a 1 percent increase in ESL students has on test scores holding all else constant

$\beta_{4}$ The effect of a 1000 dollar increase in District average income on test scores holding all else equal.



$x_{i1}$: Percentage of students qualifying for reduced-price lunch

$x_{i2}$: Number of computers

$x_{i3}$: Percentage of ESL students

$x_{i4$: District Average Income


In order to use this linear model, we check assumptions of linearity, independence, normality, and equal variance.

The following added variable plots show linearity.

```{r}
par(mfrow = c(1,4))
par(mar = c(3,2,4,0.5))
p1 <- avPlot(school.lm, variable = "Income", main = "Income")
p2 <- avPlot(school.lm, variable = "Lunch", main = "Lunch")
p3 <- avPlot(school.lm, variable = "Computer", main = "Computer")
p4 <- avPlot(school.lm, variable = "English", main = "English")

```
Because the data were collected independently and there seems to be no relationships between observation schools, we assume indepenence.

Next we check for normality. This is done by looking at a histogram of residuals. Since the histogram looks normal, our normality assumption is met.
```{recho = FALSE}

ggplot(mapping = aes(x = school.lm$residuals)) + 
  geom_histogram(bins = 15)+
  labs(title = "Histogram of Residuals", 
       x="",
       y = "")

```


Finally we check our equal variance assumption by looking at a scatterplot of fitted values vs residuals. Because there are no trends or patterns in the data, we assume equal variance. 

```{r echo = FALSE}

ggplot(mapping = aes(y = school.lm$residuals, x = school.lm$fitted.values)) + 
  geom_point() + 
  labs(title = "Scatterplot of fitted vs residuals", 
       y = "Residuals", 
       x = "Fitted Values")
```


Since all of our assumptions are met, we can proceed with the analysis.

# Model Justification and Performance Evaluations



```{r, include = FALSE}

# Cross validation
library(magrittr)
mydataset <- school
n <- round(nrow(mydataset),0)
n.cv <- 100 #Number of CV studies to run
n.test <-  round(nrow(mydataset)*0.1,0)
rpmse <- rep(x=NA, times=n.cv)
bias <- rep(x=NA, times=n.cv)
wid <- rep(x=NA, times=n.cv)
cvg <- rep(x=NA, times=n.cv)
for(cv in 1:n.cv){
  ## Select test observations
  test.obs <- sample(x=1:n, size=n.test)
  
  ## Split into test and training sets
  test.set <- mydataset[test.obs,]
  train.set <- mydataset[-test.obs,]
  
  ## Fit a lm() using the training data
  train.lm <- lm(Score~ Lunch + Computer + English + Income, data = train.set)
  
  ## Generate predictions for the test set
  my.preds <- predict.lm(train.lm, newdata=test.set, interval="prediction")
  
  ## Calculate bias
  bias[cv] <- mean(my.preds[,'fit']-test.set[['Score']])
  
  ## Calculate RPMSE
  rpmse[cv] <- (test.set[['Score']]-my.preds[,'fit'])^2 %>% mean() %>% sqrt()
  
  ## Calculate Coverage
  cvg[cv] <- ((test.set[['Score']] > my.preds[,'lwr']) & (test.set[['Score']] < my.preds[,'upr'])) %>% mean()
  
  ## Calculate Width
  wid[cv] <- (my.preds[,'upr'] - my.preds[,'lwr']) %>% mean()
  
}
hist(rpmse)
mean(rpmse)
mean(bias)
hist(bias)
mean(bias)
hist(cvg)
mean(cvg)
hist(wid)
mean(wid)
sd(school$Score)
summary(school.lm)
```



```{r}
table <- cbind(school.lm$coefficients,confint(school.lm))
table <- round(table[,1:3],2)
knitr::kable(table, col.names = c("Estimate", "Lower CI", "Upper CI"), caption = "Variable Estimates")


# RMSE/R^2 Comparison
table <- data.frame("Method" = c("Linear Model", "Bagging (160)", "Random Forest (50)", "Boosting (100)"), "R Squared" = c(0.765, 0.766, 0.751, 0.733), "RMSE" = c(19.6, 19.2, 20.2, 20.9))
knitr::kable(table)
```






```{r include = FALSE}
# Bagging model with 160 trees is dang good

#  mse is 19.22

library(ranger)
num_trees <- seq(1,500, length.out = 100)
bag.pred.error <- c()
bag.r.sqr <- c()
SumE <- 0

### Identify test sets
K = 5
possibilities = 1:nrow(school)
this.many = round(nrow(school)/K)



splits <- list()
already.used <- NA
for(i in 2:K){
  samp <- sample(possibilities, this.many, replace = FALSE)
  splits[[i]] <- samp
  possibilities = possibilities[!(possibilities %in% splits[[i]]) ]
}
splits[[1]] = possibilities

sqrt(mean(bag.pred.error))
mean(bag.r.sqr)
which.min(bag.pred.error)
min(bag.pred.error)

bag <- ranger(Score~.,data=school, num.trees = num_trees[32], mtry = ncol(school) - 1, importance = "permutation") 

library(vip)
library(ggplot2)
vip(bag)
```

```{r include = FALSE}
forest <- bag
item <- importance(forest)
library(dplyr)
var.data <- tibble("var" = names(item), "importance" = unname(item)) %>%
  arrange(desc(importance))
var.data$var <- reorder(var.data$var, -var.data$importance)
```

```{r echo = FALSE}
ggplot(var.data[1:6,], aes(var, importance)) +
  geom_bar(stat = "identity", fill = "navy") +
  theme(axis.text.x=element_text(angle=45,hjust=0.65,vjust=0.5)) +
  xlab("") +
  ylab("Variable Importance") +
  labs(caption = "Figure 3") +
  ggtitle('Bagging Model Variable Importance') +
  theme(text = element_text(size = 14),
        plot.title = element_text(hjust = 0.5))
```

```{r inclue = FALSE}

#RF


library(ranger)
num_trees <- seq(1,500, length.out = 100)
bag.pred.error <- c()
bag.r.sqr <- c()
for(i in 1:5){
  bag <- ranger(Score~.,data=school,num.trees = 47, mtry = 2, importance = "permutation") 
  bag.pred.error[i] <- bag$prediction.error
  bag.r.sqr[i] <- bag$r.squared
}

mean(bag.r.sqr)
which.min(bag.pred.error)
sqrt(min(bag.pred.error))


```

#boosting

```{r include = FALSE}
library(gbm)
library(magrittr)
### Identify test sets
K = 5
possibilities = 1:nrow(school)
this.many = round(nrow(school)/K)



splits <- list()
already.used <- NA
for(i in 2:K){
  samp <- sample(possibilities, this.many, replace = FALSE)
  splits[[i]] <- samp
  possibilities = possibilities[!(possibilities %in% splits[[i]]) ]
}
splits[[1]] = possibilities


SSE <-SumE <- 0
oob.mse.gbm <- NA
boost.rsq <- NA
for(i in 1:5){
  train.data = school[-splits[[i]],]
  test.data = school[splits[[i]], ]
  
  gbm1 <- gbm(Score~.,data=train.data, distribution = "gaussian", n.trees = 100, interaction.depth = 4, shrinkage = 0.1)  
  p = predict(gbm1, newdata = test.data)
  
  oob.mse.gbm[i] <- (p- test.data$Score)^2 %>% mean()
  boost.rsq[i] <- 1-(sum((p- test.data$Score)^2)/sum((school$Score - mean(school$Score))^2))
  
  SSE = SSE + sum((p - test.data$Score)^2)
  SumE = SumE + sum(p - test.data$Score)
}

sqrt(mean(oob.mse.gbm))
rsq = 1- SSE/(sum((school$Score - mean(school$Score))^2))
```


```{r}
# library(BART) #we'll use gbart or wbart functions. (I'm pretty sure gbart just uses wbart when you put in a continuous outcome)
# library(magrittr)
# 
# n.trees <- seq(20, 300, length.out = 10)
# oob.mse <- NA
# bart.rsq <- NA
# SSE = 0
# #120 trees i think is my best
# for(i in 1:5){
#   
#   train <- school[-splits[[i]],]
#   test <- school[splits[[i]],]
#   bart = gbart(x.train = train[,-1], y.train = train$Score, x.test = test[,-1], ntree = 400, ndpost=1000, nskip = 300, sparse = TRUE)
#   oob.mse[i] <- (bart$yhat.test.mean - test$Score)^2 %>% mean()
#   bart.rsq[i] <- 1-(sum((bart$yhat.train.mean - train$Score)^2)/sum((train$Score - mean(train$Score))^2))
# 
#     SSE = SSE + sum((bart$yhat.test.mean - test.data$Score)^2)
# 
# }
# 
# # bart = gbart(x.train = train[,-1], y.train = train$Score, x.test = test[,-1], ntree = 600, ndpost=4000, nskip = 300, sparse = TRUE)
# 
# 
# sqrt(mean(oob.mse))
# rsq = 1- SSE/(sum((school$Score - mean(school$Score))^2))
# bart.rsq
# rsq
```

